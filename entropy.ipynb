{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Epoch 1: Train Loss = 0.2647, Test Loss = 0.0826, Accuracy = 97.44%\n",
      "Epoch 2: Train Loss = 0.0758, Test Loss = 0.0543, Accuracy = 98.22%\n",
      "Epoch 3: Train Loss = 0.0544, Test Loss = 0.0406, Accuracy = 98.58%\n",
      "Epoch 4: Train Loss = 0.0446, Test Loss = 0.0372, Accuracy = 98.71%\n",
      "Epoch 5: Train Loss = 0.0379, Test Loss = 0.0422, Accuracy = 98.66%\n",
      "Epoch 6: Train Loss = 0.0315, Test Loss = 0.0401, Accuracy = 98.70%\n",
      "Epoch 7: Train Loss = 0.0274, Test Loss = 0.0385, Accuracy = 98.78%\n",
      "Epoch 8: Train Loss = 0.0229, Test Loss = 0.0406, Accuracy = 98.91%\n",
      "Epoch 9: Train Loss = 0.0212, Test Loss = 0.0350, Accuracy = 98.97%\n",
      "Epoch 10: Train Loss = 0.0197, Test Loss = 0.0390, Accuracy = 98.79%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Definição da arquitetura LeNet-5\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)  # 1 canal (imagem em grayscale)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # Ajuste para MNIST (28x28)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)  # 10 classes (dígitos de 0 a 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Preparação do dataset MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalização para [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Configuração do dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Inicialização do modelo, função de perda e otimizador\n",
    "model = LeNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Função de treino\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Zerar gradientes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass e atualização dos pesos\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# Função de teste\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "\n",
    "            # Acertos\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy\n",
    "\n",
    "# Loop de treinamento\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    test_loss, accuracy = test(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}, Accuracy = {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6847, Test Loss = 0.0880, Accuracy = 97.42%\n",
      "Epoch 2: Train Loss = 0.4374, Test Loss = 0.0482, Accuracy = 98.46%\n",
      "Epoch 3: Train Loss = 0.3980, Test Loss = 0.0367, Accuracy = 98.85%\n",
      "Epoch 4: Train Loss = 0.3807, Test Loss = 0.0393, Accuracy = 98.74%\n",
      "Epoch 5: Train Loss = 0.3718, Test Loss = 0.0376, Accuracy = 98.69%\n",
      "Epoch 6: Train Loss = 0.3625, Test Loss = 0.0341, Accuracy = 98.84%\n",
      "Epoch 7: Train Loss = 0.3529, Test Loss = 0.0330, Accuracy = 98.95%\n",
      "Epoch 8: Train Loss = 0.3440, Test Loss = 0.0353, Accuracy = 98.79%\n",
      "Epoch 9: Train Loss = 0.3546, Test Loss = 0.0320, Accuracy = 99.06%\n",
      "Epoch 10: Train Loss = 0.3323, Test Loss = 0.0332, Accuracy = 98.95%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Definição da arquitetura LeNet-5 (mesma anterior)\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Função para calcular a entropia dos pesos\n",
    "def entropy_regularization(model):\n",
    "    entropy = 0.0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            # Normaliza os pesos para criar uma distribuição discreta\n",
    "            weights = param.view(-1)\n",
    "            prob = torch.abs(weights) / torch.sum(torch.abs(weights))\n",
    "            prob = prob + 1e-10  # Evitar log(0)\n",
    "            entropy -= torch.sum(prob * torch.log(prob))\n",
    "    return entropy\n",
    "\n",
    "# Configurações e dados\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loop de treinamento com regularização por entropia\n",
    "epochs = 10\n",
    "lambda_entropy = 0.01  # Peso do termo de regularização\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Zerar gradientes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Regularização por entropia\n",
    "        entropy_loss = entropy_regularization(model)\n",
    "        total_loss = loss + lambda_entropy * entropy_loss\n",
    "\n",
    "        # Backward pass e otimização\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "    # Avaliação no conjunto de teste\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"Epoch {epoch}: Train Loss = {running_loss / len(train_loader):.4f}, \"\n",
    "          f\"Test Loss = {test_loss:.4f}, Accuracy = {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 37.0150, Test Loss = 0.2120, Accuracy = 94.54%, Entropy = 36.5773\n",
      "Epoch 2: Train Loss = 27.7074, Test Loss = 0.2224, Accuracy = 93.83%, Entropy = 27.4645\n",
      "Epoch 3: Train Loss = 24.6936, Test Loss = 0.2456, Accuracy = 93.02%, Entropy = 24.4586\n",
      "Epoch 4: Train Loss = 23.5434, Test Loss = 0.2491, Accuracy = 92.78%, Entropy = 23.2962\n",
      "Epoch 5: Train Loss = 22.3529, Test Loss = 0.2705, Accuracy = 92.37%, Entropy = 22.0793\n",
      "Epoch 6: Train Loss = 21.3779, Test Loss = 0.2597, Accuracy = 92.31%, Entropy = 21.0858\n",
      "Epoch 7: Train Loss = 20.6839, Test Loss = 0.2533, Accuracy = 92.26%, Entropy = 20.4121\n",
      "Epoch 8: Train Loss = 20.0685, Test Loss = 0.2307, Accuracy = 93.02%, Entropy = 19.8167\n",
      "Epoch 9: Train Loss = 19.6857, Test Loss = 0.2236, Accuracy = 93.19%, Entropy = 19.4508\n",
      "Epoch 10: Train Loss = 19.5199, Test Loss = 0.2062, Accuracy = 93.68%, Entropy = 19.2743\n",
      "Epoch 11: Train Loss = 19.2240, Test Loss = 0.2255, Accuracy = 92.94%, Entropy = 18.9781\n",
      "Epoch 12: Train Loss = 18.9138, Test Loss = 0.2294, Accuracy = 93.42%, Entropy = 18.6942\n",
      "Epoch 13: Train Loss = 18.6589, Test Loss = 0.1985, Accuracy = 94.13%, Entropy = 18.4450\n",
      "Epoch 14: Train Loss = 18.4654, Test Loss = 0.1865, Accuracy = 94.53%, Entropy = 18.2581\n",
      "Epoch 15: Train Loss = 18.4923, Test Loss = 0.1973, Accuracy = 93.98%, Entropy = 18.2844\n",
      "Epoch 16: Train Loss = 18.1521, Test Loss = 0.1735, Accuracy = 94.80%, Entropy = 17.9547\n",
      "Epoch 17: Train Loss = 17.9095, Test Loss = 0.1872, Accuracy = 94.14%, Entropy = 17.7163\n",
      "Epoch 18: Train Loss = 17.6654, Test Loss = 0.1750, Accuracy = 94.74%, Entropy = 17.4749\n",
      "Epoch 19: Train Loss = 17.6250, Test Loss = 0.1607, Accuracy = 95.20%, Entropy = 17.4256\n",
      "Epoch 20: Train Loss = 17.5846, Test Loss = 0.1818, Accuracy = 94.45%, Entropy = 17.3979\n",
      "Epoch 21: Train Loss = 17.1553, Test Loss = 0.1598, Accuracy = 95.34%, Entropy = 16.9656\n",
      "Epoch 22: Train Loss = 16.9000, Test Loss = 0.1627, Accuracy = 95.06%, Entropy = 16.7189\n",
      "Epoch 23: Train Loss = 16.7952, Test Loss = 0.1549, Accuracy = 95.56%, Entropy = 16.6126\n",
      "Epoch 24: Train Loss = 16.7197, Test Loss = 0.1464, Accuracy = 95.63%, Entropy = 16.5423\n",
      "Epoch 25: Train Loss = 16.5929, Test Loss = 0.1438, Accuracy = 95.81%, Entropy = 16.4192\n",
      "Epoch 26: Train Loss = 17.0942, Test Loss = 0.1601, Accuracy = 95.26%, Entropy = 16.9212\n",
      "Epoch 27: Train Loss = 16.5069, Test Loss = 0.1489, Accuracy = 95.51%, Entropy = 16.3341\n",
      "Epoch 28: Train Loss = 16.3340, Test Loss = 0.1495, Accuracy = 95.40%, Entropy = 16.1623\n",
      "Epoch 29: Train Loss = 16.3611, Test Loss = 0.1466, Accuracy = 95.69%, Entropy = 16.1939\n",
      "Epoch 30: Train Loss = 16.2719, Test Loss = 0.1505, Accuracy = 95.59%, Entropy = 16.1032\n",
      "Epoch 31: Train Loss = 16.0936, Test Loss = 0.1444, Accuracy = 95.63%, Entropy = 15.9253\n",
      "Epoch 32: Train Loss = 16.0214, Test Loss = 0.1384, Accuracy = 95.75%, Entropy = 15.8594\n",
      "Epoch 33: Train Loss = 15.9506, Test Loss = 0.1420, Accuracy = 95.68%, Entropy = 15.7887\n",
      "Epoch 34: Train Loss = 15.9833, Test Loss = 0.1330, Accuracy = 96.14%, Entropy = 15.8229\n",
      "Epoch 35: Train Loss = 16.6356, Test Loss = 0.1654, Accuracy = 95.10%, Entropy = 16.4528\n",
      "Epoch 36: Train Loss = 16.9056, Test Loss = 0.1378, Accuracy = 95.83%, Entropy = 16.7354\n",
      "Epoch 37: Train Loss = 16.7064, Test Loss = 0.1743, Accuracy = 95.20%, Entropy = 16.5415\n",
      "Epoch 38: Train Loss = 16.5522, Test Loss = 0.1410, Accuracy = 95.57%, Entropy = 16.3868\n",
      "Epoch 39: Train Loss = 16.3745, Test Loss = 0.1472, Accuracy = 95.51%, Entropy = 16.2129\n",
      "Epoch 40: Train Loss = 16.1681, Test Loss = 0.1398, Accuracy = 95.91%, Entropy = 16.0085\n",
      "Epoch 41: Train Loss = 15.9645, Test Loss = 0.1473, Accuracy = 95.41%, Entropy = 15.8065\n",
      "Epoch 42: Train Loss = 15.7566, Test Loss = 0.1386, Accuracy = 95.74%, Entropy = 15.6000\n",
      "Epoch 43: Train Loss = 15.6068, Test Loss = 0.1441, Accuracy = 95.46%, Entropy = 15.4511\n",
      "Epoch 44: Train Loss = 15.5504, Test Loss = 0.1268, Accuracy = 96.25%, Entropy = 15.3967\n",
      "Epoch 45: Train Loss = 15.4839, Test Loss = 0.1468, Accuracy = 95.54%, Entropy = 15.3255\n",
      "Epoch 46: Train Loss = 15.3916, Test Loss = 0.1688, Accuracy = 94.88%, Entropy = 15.2437\n",
      "Epoch 47: Train Loss = 15.3664, Test Loss = 0.1285, Accuracy = 96.14%, Entropy = 15.2135\n",
      "Epoch 48: Train Loss = 15.3362, Test Loss = 0.1423, Accuracy = 95.79%, Entropy = 15.1643\n",
      "Epoch 49: Train Loss = 15.3116, Test Loss = 0.1221, Accuracy = 96.26%, Entropy = 15.1610\n",
      "Epoch 50: Train Loss = 15.3241, Test Loss = 0.1270, Accuracy = 96.09%, Entropy = 15.1747\n",
      "\n",
      "Evolução da Entropia ao longo das épocas:\n",
      "Epoch 1: Entropy = 36.5773\n",
      "Epoch 2: Entropy = 27.4645\n",
      "Epoch 3: Entropy = 24.4586\n",
      "Epoch 4: Entropy = 23.2962\n",
      "Epoch 5: Entropy = 22.0793\n",
      "Epoch 6: Entropy = 21.0858\n",
      "Epoch 7: Entropy = 20.4121\n",
      "Epoch 8: Entropy = 19.8167\n",
      "Epoch 9: Entropy = 19.4508\n",
      "Epoch 10: Entropy = 19.2743\n",
      "Epoch 11: Entropy = 18.9781\n",
      "Epoch 12: Entropy = 18.6942\n",
      "Epoch 13: Entropy = 18.4450\n",
      "Epoch 14: Entropy = 18.2581\n",
      "Epoch 15: Entropy = 18.2844\n",
      "Epoch 16: Entropy = 17.9547\n",
      "Epoch 17: Entropy = 17.7163\n",
      "Epoch 18: Entropy = 17.4749\n",
      "Epoch 19: Entropy = 17.4256\n",
      "Epoch 20: Entropy = 17.3979\n",
      "Epoch 21: Entropy = 16.9656\n",
      "Epoch 22: Entropy = 16.7189\n",
      "Epoch 23: Entropy = 16.6126\n",
      "Epoch 24: Entropy = 16.5423\n",
      "Epoch 25: Entropy = 16.4192\n",
      "Epoch 26: Entropy = 16.9212\n",
      "Epoch 27: Entropy = 16.3341\n",
      "Epoch 28: Entropy = 16.1623\n",
      "Epoch 29: Entropy = 16.1939\n",
      "Epoch 30: Entropy = 16.1032\n",
      "Epoch 31: Entropy = 15.9253\n",
      "Epoch 32: Entropy = 15.8594\n",
      "Epoch 33: Entropy = 15.7887\n",
      "Epoch 34: Entropy = 15.8229\n",
      "Epoch 35: Entropy = 16.4528\n",
      "Epoch 36: Entropy = 16.7354\n",
      "Epoch 37: Entropy = 16.5415\n",
      "Epoch 38: Entropy = 16.3868\n",
      "Epoch 39: Entropy = 16.2129\n",
      "Epoch 40: Entropy = 16.0085\n",
      "Epoch 41: Entropy = 15.8065\n",
      "Epoch 42: Entropy = 15.6000\n",
      "Epoch 43: Entropy = 15.4511\n",
      "Epoch 44: Entropy = 15.3967\n",
      "Epoch 45: Entropy = 15.3255\n",
      "Epoch 46: Entropy = 15.2437\n",
      "Epoch 47: Entropy = 15.2135\n",
      "Epoch 48: Entropy = 15.1643\n",
      "Epoch 49: Entropy = 15.1610\n",
      "Epoch 50: Entropy = 15.1747\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Definição da arquitetura LeNet-5\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Função para calcular a entropia dos pesos\n",
    "def entropy_regularization(model):\n",
    "    total_entropy = 0.0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            # Normaliza os pesos para criar uma distribuição discreta\n",
    "            weights = param.view(-1)\n",
    "            prob = torch.abs(weights) / torch.sum(torch.abs(weights))\n",
    "            prob = prob + 1e-10  # Evitar log(0)\n",
    "            total_entropy -= torch.sum(prob * torch.log(prob))\n",
    "    return total_entropy\n",
    "\n",
    "# Configurações de dados\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Configuração do dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loop de treinamento com monitoramento da entropia\n",
    "epochs = 50\n",
    "lambda_entropy = 1  # Peso da regularização\n",
    "entropies = []  # Lista para salvar entropias ao longo das épocas\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    epoch_entropy = 0.5\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Zerar gradientes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Regularização por entropia\n",
    "        entropy_loss = entropy_regularization(model)\n",
    "        total_loss = loss + lambda_entropy * entropy_loss\n",
    "\n",
    "        # Backward pass e otimização\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "        epoch_entropy += entropy_loss.item()\n",
    "\n",
    "    # Salvar entropia média da época\n",
    "    epoch_entropy /= len(train_loader)\n",
    "    entropies.append(epoch_entropy)\n",
    "\n",
    "    # Avaliação no conjunto de teste\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    # Exibir métricas da época\n",
    "    print(f\"Epoch {epoch}: Train Loss = {running_loss / len(train_loader):.4f}, \"\n",
    "          f\"Test Loss = {test_loss:.4f}, Accuracy = {accuracy:.2f}%, Entropy = {epoch_entropy:.4f}\")\n",
    "\n",
    "# Exibir evolução da entropia\n",
    "print(\"\\nEvolução da Entropia ao longo das épocas:\")\n",
    "for epoch, entropy in enumerate(entropies, 1):\n",
    "    print(f\"Epoch {epoch}: Entropy = {entropy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "p_brasil = 0.5\n",
    "p_espanha = 0.3\n",
    "p_franca = 0.2\n",
    "\n",
    "i_brasil = np.log2(1/p_brasil)\n",
    "i_espanha = np.log2(1/p_espanha)\n",
    "i_franca = np.log2(1/p_franca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Densidade em cada intervalo: [0.26904525 0.5380905  0.5380905  0.         0.26904525 0.26904525\n",
      " 0.26904525 0.26904525 0.         0.26904525]\n",
      "Bordas dos intervalos: [-1.43764164 -1.06595695 -0.69427226 -0.32258757  0.04909711  0.4207818\n",
      "  0.79246649  1.16415118  1.53583587  1.90752056  2.27920525]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dados para o histograma\n",
    "data = np.random.randn(10)\n",
    "\n",
    "counts, bin_edges = np.histogram(data, bins=10, density=True)\n",
    "\n",
    "print(\"Densidade em cada intervalo:\", counts)\n",
    "print(\"Bordas dos intervalos:\", bin_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.94924999,  2.27920525, -0.87306693, -1.43764164,  1.18779224,\n",
       "       -0.54371496,  1.08192178,  0.62692954, -0.47510681,  0.23443902])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.4854752972273344)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_brasil * i_brasil + p_espanha * i_espanha + p_franca*i_franca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.058893689053568)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_brasil + i_espanha + i_franca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "model = nn.Linear(3, 4)\n",
    "\n",
    "size_model = 0\n",
    "for param in model.parameters():\n",
    "    size_model += len(param.view(-1)) * param.element_size()\n",
    "size_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "model = nn.Linear(3, 4)\n",
    "\n",
    "\n",
    "def calculate_model_size(model, include_grad=False):\n",
    "    size_model = 0\n",
    "    for param in model.parameters():\n",
    "        size_model += param.numel() * param.element_size()\n",
    "        if include_grad:\n",
    "            size_model += param.grad.numel() * param.grad.element_size()\n",
    "    return size_model\n",
    "\n",
    "calculate_model_size(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
